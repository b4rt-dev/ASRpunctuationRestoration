{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ASR Punctuation Restoration Experiments\n",
    "By Bart Pleiter S4752740 for the course ASR 2021-2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "from tensorflow.keras import mixed_precision\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "from transformers import TFBertForMaskedLM\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use mixed precision to speed up training on my RTX3050\n",
    "mixed_precision.set_global_policy('mixed_float16')\n",
    "\n",
    "memoryLimit = 7000 # 7.0GB\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        tf.config.experimental.set_virtual_device_configuration(gpus[0], [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=memoryLimit)])\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "batchSize = 64\n",
    "numLabels = 4\n",
    "segmentSize = 32 # MUST be an even number\n",
    "dropout = 0.3\n",
    "learningRate = 1e-5\n",
    "epochs = 3\n",
    "vocabSize = 30522 # of the BERT model\n",
    "\n",
    "# labels\n",
    "LABEL_NOTHING = 0\n",
    "LABEL_COMMA = 1\n",
    "LABEL_PERIOD = 2\n",
    "LABEL_QUESTION = 3\n",
    "\n",
    "labelNames = [\"O\", \"COMMA\", \"PERIOD\", \"QUESTION\"]\n",
    "\n",
    "# encode the punctuation label as a number\n",
    "punctEncode = {\n",
    "    'O': LABEL_NOTHING,\n",
    "    'COMMA': LABEL_COMMA,\n",
    "    'PERIOD': LABEL_PERIOD,\n",
    "    'QUESTION': LABEL_QUESTION\n",
    "}\n",
    "\n",
    "# decode the label for printing purposes\n",
    "punctDecode = {v: k for k, v in punctEncode.items()}\n",
    "\n",
    "# tokenizer that was used to train the pre-trained transformer model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insertTarget(x, segment_size):\n",
    "    # creates segments of surrounding words for each word in x.\n",
    "    # inserts a zero token ([PAD]) halfway the segment, right after the inserted token.\n",
    "    # for the first segmentSize/2 tokens, the end of the data is used, as if the text loops\n",
    "    X = []\n",
    "    x_pad = x[-((segment_size-1)//2-1):]+x+x[:segment_size//2]\n",
    "\n",
    "    for i in range(len(x_pad)-segment_size+2):\n",
    "        segment = x_pad[i:i+segment_size-1]\n",
    "        segment.insert((segment_size-1)//2, 0)\n",
    "        X.append(segment)\n",
    "\n",
    "    return np.array(X)\n",
    "\n",
    "def encodeRawText(text):\n",
    "    # splits the text on spaces and creates labels for each created token\n",
    "    # the resulting token list will have no punctuation anymore\n",
    "    splitOnSpace = text.split(' ')\n",
    "    X = []\n",
    "    Y = []\n",
    "    \n",
    "    for word in splitOnSpace:\n",
    "        if len(word) > 0: # skip empty tokens\n",
    "            # look for tokens at the end of the word\n",
    "            # remove if found when appending to X\n",
    "            # also make everything lowercase\n",
    "            if word[-1] == '.':\n",
    "                X.append(word.lower()[:-1])\n",
    "                Y.append(LABEL_PERIOD)\n",
    "            elif word[-1] == ',':\n",
    "                X.append(word.lower()[:-1])\n",
    "                Y.append(LABEL_COMMA)\n",
    "            elif word[-1] == '?':\n",
    "                X.append(word.lower()[:-1])\n",
    "                Y.append(LABEL_QUESTION)\n",
    "            else:\n",
    "                X.append(word.lower())\n",
    "                Y.append(LABEL_NOTHING)\n",
    "                \n",
    "    return X, Y\n",
    "\n",
    "def prepareDataForModel(words, labels, tokenizer):\n",
    "    # returns a list of segments of token IDs for X\n",
    "    #  and a list of label tokens for y, corresponding to the segment in X\n",
    "    #  and a list of the unsegmented token IDs, for easier reconstruction\n",
    "    X = []\n",
    "    Y = []\n",
    "    for word, label in zip(words, labels):\n",
    "        y = [label]\n",
    "        # retokenize x\n",
    "        x = tokenizer.wordpiece_tokenizer.tokenize(word)\n",
    "        # encode x\n",
    "        x = tokenizer.convert_tokens_to_ids(x)\n",
    "\n",
    "        # do not add if tokenize failed\n",
    "        if len(x) > 0:\n",
    "            # if multiple tokens, create multiple labels of 0\n",
    "            #  set the last one to the real label\n",
    "            if len(x) > 1:\n",
    "                y = (len(x)-1)*[0]+y\n",
    "            X += x\n",
    "            Y += y\n",
    "                \n",
    "    # create segments for X, and return together with Y, and the unsegmented tokens\n",
    "    # return as Numpy array\n",
    "    return np.array(insertTarget(X, segmentSize)), np.array(Y), X\n",
    "\n",
    "def getTokenFromSegment(segment):\n",
    "    # is always at the same place of the segment (assumes even numbers!)\n",
    "    return segment[segmentSize//2-2]\n",
    "\n",
    "def reconstructText(tokenList, labels, tokenizer):\n",
    "    # reconstructs text by detokenizing and applying the given labels\n",
    "    tokens = tokenizer.convert_ids_to_tokens(tokenList)\n",
    "    \n",
    "    reconstructedText = \"\"\n",
    "    for tok, label in zip(tokens, labels):\n",
    "        # no space in between if second token starts with '##'\n",
    "        if tok.startswith(\"##\"):\n",
    "            reconstructedText += tok[2:]\n",
    "        else:\n",
    "            reconstructedText += \" \" + tok\n",
    "        \n",
    "        # add the punctuation from the label\n",
    "        if label == LABEL_COMMA:\n",
    "            reconstructedText += \",\"\n",
    "        elif label == LABEL_PERIOD:\n",
    "            reconstructedText += \".\"\n",
    "        elif label == LABEL_QUESTION:\n",
    "            reconstructedText += \"?\"\n",
    "                \n",
    "    reconstructedText = reconstructedText[1:] # skip the first space\n",
    "    \n",
    "    return reconstructedText\n",
    "    \n",
    "def loadDataFromFile(path):\n",
    "    # creates a dataset from a text file, which can be used on the model\n",
    "    # returns a list of segments of token IDs for X\n",
    "    #  and a list of label tokens for y, corresponding to the segment in X\n",
    "    #  and a list of the unsegmented token IDs, for easier reconstruction\n",
    "    text = \"\"\n",
    "    with open(path, 'r') as file:\n",
    "        text = file.read()\n",
    "        \n",
    "    X, Y = encodeRawText(text)\n",
    "    dataX, dataY, dataTokens = prepareDataForModel(X, Y, tokenizer)\n",
    "    \n",
    "    return dataX, dataY, dataTokens\n",
    "\n",
    "def loadDataFromTEDtalkDataset(path, tokenizer):\n",
    "    # loads the data from the TED talk dataset, which is already pre-processed\n",
    "    # returns a list of segments of token IDs for X\n",
    "    #  and a list of label tokens for y, corresponding to the segment in X\n",
    "    #  and a list of the unsegmented token IDs, for easier reconstruction\n",
    "    X = []\n",
    "    Y = []\n",
    "    with open(path, \"rb\") as file:\n",
    "        for line in file:\n",
    "            # dataset uses \\r\\n for newlines\n",
    "            word, punc = line.decode('utf-8', errors='ignore').replace('\\r\\n', '').split('\\t')\n",
    "            # encode y\n",
    "            y = [punctEncode[punc]]\n",
    "            # retokenize x\n",
    "            x = tokenizer.wordpiece_tokenizer.tokenize(word)\n",
    "            # encode x\n",
    "            x = tokenizer.convert_tokens_to_ids(x)\n",
    "            \n",
    "            # do not add if tokenize failed\n",
    "            if len(x) > 0:\n",
    "                # if multiple tokens, create multiple labels of 0\n",
    "                #  set the last one to the real label\n",
    "                if len(x) > 1:\n",
    "                    y = (len(x)-1)*[0]+y\n",
    "                X += x\n",
    "                Y += y\n",
    "                \n",
    "    # create segments for X, and return together with Y, and the unsegmented tokens\n",
    "    # return as Numpy array\n",
    "    return np.array(insertTarget(X, segmentSize)), np.array(Y), X\n",
    "\n",
    "def predictText(text, model, tokenizer):\n",
    "    # predicts the punctuation for a given text and print the resulting text\n",
    "    \n",
    "    # pre-process\n",
    "    X, Y = encodeRawText(text)\n",
    "    dataX, dataY, dataTokens = prepareDataForModel(X, Y, tokenizer)\n",
    "    \n",
    "    # get results\n",
    "    results = model.predict(dataX)\n",
    "    \n",
    "    # select best class for each token\n",
    "    resultY = results.argmax(axis=1)\n",
    "    \n",
    "    # reconstruct text with predicted tokens\n",
    "    resText = reconstructText(dataTokens, resultY, tokenizer)\n",
    "    \n",
    "    print(resText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluates the predicted results\n",
    "def evaluateResults(Ytrue, Ypred):    \n",
    "    print(classification_report(Ytrue, Ypred, target_names=labelNames))\n",
    "    \n",
    "    confusionMatrix = confusion_matrix(Ytrue, Ypred)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=confusionMatrix, display_labels=labelNames)\n",
    "    print(\"Confusion matrix:\")\n",
    "    disp.plot()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct the network\n",
    "bert_input = tf.keras.Input(shape=(segmentSize), dtype='int32', name='bert_input')\n",
    "x = TFBertForMaskedLM.from_pretrained(\"bert-base-uncased\")(bert_input)[0]\n",
    "print('dtype BERT layer: %s' % x.dtype.name)\n",
    "x = tf.keras.layers.Reshape((segmentSize*vocabSize,))(x)\n",
    "print('dtype reshape layer: %s' % x.dtype.name)\n",
    "x = tf.keras.layers.Dropout(dropout, name=\"dropout\")(x)\n",
    "print('dtype dropout layer: %s' % x.dtype.name)\n",
    "x = tf.keras.layers.Dense(len(punctEncode), name='dense')(x)\n",
    "print('dtype dense layer: %s' % x.dtype.name)\n",
    "# dtype float32 because of mixed precision\n",
    "dense_out = tf.keras.layers.Activation('softmax', dtype='float32', name='softmax')(x)\n",
    "print('dtype softmax layer: %s' % dense_out.dtype.name)\n",
    "\n",
    "model = tf.keras.Model(bert_input, dense_out, name='model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the model architecture\n",
    "tf.keras.utils.plot_model(model, show_shapes=True, dpi=48)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the data for training\n",
    "Xtrain, Ytrain, _ = loadDataFromTEDtalkDataset(\"IWSLT2012data/train2012\", tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xval, Yval, valTokens = loadDataFromTEDtalkDataset(\"IWSLT2012data/dev2012\", tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the network\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=learningRate),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "    metrics=[tf.keras.metrics.SparseCategoricalAccuracy()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training the data\n",
    "history = model.fit(\n",
    "    Xtrain,\n",
    "    Ytrain,\n",
    "    epochs=epochs,\n",
    "    batch_size = batchSize,\n",
    "    validation_data=(Xval, Yval)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the fitting history\n",
    "history.history\n",
    "\n",
    "# plot the loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate output on validation data\n",
    "valPredict = model.predict(Xval)\n",
    "\n",
    "# select best class for each token\n",
    "resultYval = valPredict.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluateResults(Yval, resultYval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the data for testing\n",
    "Xtest, Ytest, testTokens = loadDataFromTEDtalkDataset(\"IWSLT2012data/test2011\", tokenizer)\n",
    "Xtestasr, Ytestasr, testasrTokens = loadDataFromTEDtalkDataset(\"IWSLT2012data/test2011asr\", tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate output on test data\n",
    "testPredict = model.predict(Xtest)\n",
    "testPredictasr = model.predict(Xtestasr)\n",
    "\n",
    "# select best class for each token\n",
    "resultYtest = testPredict.argmax(axis=1)\n",
    "resultYtestasr = testPredictasr.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Test set:\")\n",
    "evaluateResults(Ytest, resultYtest)\n",
    "\n",
    "print(\"Test ASR set:\")\n",
    "evaluateResults(Ytestasr, resultYtestasr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate on test data\n",
    "#resultsTest = model.evaluate(Xtest, Ytest, batch_size=batchSize)\n",
    "#print(\"test loss, test acc:\", resultsTest)\n",
    "\n",
    "#resultsTestasr = model.evaluate(Xtest, Ytest, batch_size=batchSize)\n",
    "#print(\"test (asr) loss, test (asr) acc:\", resultsTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the restoration on the test sets\n",
    "# get results\n",
    "#restoreTest = model.predict(Xtest)\n",
    "#restoreTestasr = model.predict(Xtestasr)\n",
    "\n",
    "# select best class for each token\n",
    "#resultYtest = restoreTest.argmax(axis=1)\n",
    "#resultYtestasr = restoreTestasr.argmax(axis=1)\n",
    "\n",
    "# reconstruct text with predicted tokens\n",
    "#resTextTest = reconstructText(testTokens, resultYtest, tokenizer)\n",
    "#resTextTestasr = reconstructText(testasrTokens, resultYtestasr, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(resTextTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(resTextTestasr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example restoration on paragraph from a story\n",
    "#toPredict = \"Then he sat down and began to reflect. In the morning he must find seconds. Whom should he choose? He searched his mind for the most important and celebrated names of his acquaintance. At last he decided on the Marquis de la Tour-Noire and Colonel Bourdin, an aristocrat and a soldier; they would do excellently. Their names would look well in the papers. He realised that he was thirsty, and drank three glasses of water one after the other; then he began to walk up and down again. He felt full of energy. If he played the gallant, showed himself determined, insisted on the most strict and dangerous arrangements, demanded a serious duel, a thoroughly serious duel, a positively terrible duel, his adversary would probably retire an apologist.\"\n",
    "#predictText(toPredict, model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
