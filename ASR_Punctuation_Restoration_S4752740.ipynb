{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ASR Punctuation Restoration Experiments\n",
    "By Bart Pleiter S4752740 for the course ASR 2021-2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "from tensorflow.keras import mixed_precision\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow.keras.utils import Sequence\n",
    "import tensorflow.keras.backend as kb\n",
    "\n",
    "#import tensorflow_hub as hub\n",
    "#import tokenization\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "from transformers import TFBertModel\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use mixed precision to speed up training on my RTX3050\n",
    "mixed_precision.set_global_policy('mixed_float16')\n",
    "\n",
    "memoryLimit = 7000 # 7.0GB\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        tf.config.experimental.set_virtual_device_configuration(gpus[0], [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=memoryLimit)])\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flags\n",
    "RECREATE_PICKLE = False # recreate the dataset prepared with the transformer's tokenizer\n",
    "\n",
    "# hyperparameters\n",
    "batchSize = 8 # more than 8 requires >7GB VRAM\n",
    "numLabels = 4\n",
    "#segmentSize = 32 # MUST be an even number\n",
    "seqLen = 512\n",
    "dropout = 0.2\n",
    "learningRate = 3e-5\n",
    "epochs = 4\n",
    "#vocabSize = 30522 # of the BERT model\n",
    "seqShift = 32\n",
    "\n",
    "# labels\n",
    "LABEL_NOTHING = 0\n",
    "LABEL_COMMA = 1\n",
    "LABEL_PERIOD = 2\n",
    "LABEL_QUESTION = 3\n",
    "\n",
    "labelNames = [\"O\", \"COMMA\", \"PERIOD\", \"QUESTION\"]\n",
    "\n",
    "# encode the punctuation label as a number\n",
    "punctEncode = {\n",
    "    'O': LABEL_NOTHING,\n",
    "    'COMMA': LABEL_COMMA,\n",
    "    'PERIOD': LABEL_PERIOD,\n",
    "    'QUESTION': LABEL_QUESTION\n",
    "}\n",
    "\n",
    "# decode the label for printing purposes\n",
    "punctDecode = {v: k for k, v in punctEncode.items()}\n",
    "\n",
    "modelName = \"bert-base-uncased\"\n",
    "\n",
    "dataPath = \"Data/\"\n",
    "\n",
    "# setup BERT and its tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_layer = TFBertModel.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RECREATE_PICKLE:\n",
    "    # loading data\n",
    "    with open(dataPath + 'train_texts.txt', 'r', encoding='utf-8') as f:\n",
    "        train_text = f.readlines()\n",
    "    with open(dataPath + 'dev_texts.txt', 'r', encoding='utf-8') as f:\n",
    "        valid_text = f.readlines()\n",
    "    with open(dataPath + 'test_texts_2012.txt', 'r', encoding='utf-8') as f:\n",
    "        test_text = f.readlines()\n",
    "\n",
    "    # put all datasets together for easy batch operations\n",
    "    datasets = train_text, valid_text, test_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data for the model\n",
    "\n",
    "def clean_text(text):\n",
    "    \n",
    "    # replacing special tokens\n",
    "    # rules used from neural-punctuator by attilanagy234\n",
    "    text = text.replace('!', '.')\n",
    "    text = text.replace(':', ',')\n",
    "    text = text.replace('--', ',')\n",
    "    \n",
    "    reg = \"(?<=[a-zA-Z])-(?=[a-zA-Z]{2,})\"\n",
    "    r = re.compile(reg, re.DOTALL)\n",
    "    text = r.sub(' ', text)\n",
    "    \n",
    "    text = re.sub(r'\\s-\\s', ' , ', text)\n",
    "    \n",
    "    text = text.replace(';', '.')\n",
    "    text = text.replace(' ,', ',')\n",
    "    text = text.replace('â™«', '')\n",
    "    text = text.replace('...', '')\n",
    "    text = text.replace('.\\\"', ',')\n",
    "    text = text.replace('\"', ',')\n",
    "\n",
    "    text = re.sub(r'--\\s?--', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    text = re.sub(r',\\s?,', ',', text)\n",
    "    text = re.sub(r',\\s?\\.', '.', text)\n",
    "    text = re.sub(r'\\?\\s?\\.', '?', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    text = re.sub(r'\\s+\\?', '?', text)\n",
    "    text = re.sub(r'\\s+,', ',', text)\n",
    "    text = re.sub(r'\\.[\\s+\\.]+', '. ', text)\n",
    "    text = re.sub(r'\\s+\\.', '.', text)\n",
    "    \n",
    "    return text.strip().lower()\n",
    "\n",
    "target_token2id = {t: tokenizer.encode(t)[-2] for t in \",.?\"}\n",
    "\n",
    "target_ids = list(target_token2id.values())\n",
    "target_ids\n",
    "\n",
    "id2target = {\n",
    "    0: 0,\n",
    "    -1: -1,\n",
    "}\n",
    "\n",
    "for i, ti in enumerate(target_ids):\n",
    "    id2target[ti] = i+1\n",
    "\n",
    "def create_target(text):\n",
    "    encoded_words, targets = [], []\n",
    "    \n",
    "    words = text.split(' ')\n",
    "\n",
    "    for word in words:\n",
    "        target = 0\n",
    "        for target_token, target_id in target_token2id.items():\n",
    "            if word.endswith(target_token):\n",
    "                word = word.rstrip(target_token)\n",
    "                target = id2target[target_id]\n",
    "\n",
    "        encoded_word = tokenizer.encode(word, add_special_tokens=False)\n",
    "        \n",
    "        for w in encoded_word:\n",
    "            encoded_words.append(w)\n",
    "        for _ in range(len(encoded_word)-1):\n",
    "            targets.append(-1)\n",
    "        targets.append(target)\n",
    "        \n",
    "        #print([tokenizer._convert_id_to_token(ew) for ew in encoded_word], target)\n",
    "        assert(len(encoded_word)>0)\n",
    "\n",
    "    encoded_words = [tokenizer.cls_token_id or tokenizer.bos_token_id] +\\\n",
    "                    encoded_words +\\\n",
    "                    [tokenizer.sep_token_id or tokenizer.eos_token_id]\n",
    "    targets = [-1] + targets + [-1]\n",
    "    \n",
    "    return encoded_words, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RECREATE_PICKLE:\n",
    "    # clean the special characters from the texts\n",
    "    datasets = [[clean_text(text) for text in ds] for ds in datasets]\n",
    "\n",
    "    # encode the texts and generate labels\n",
    "    encoded_texts, targets = [], []\n",
    "\n",
    "    for ds in datasets:\n",
    "        x = list(zip(*(create_target(ts) for ts in tqdm(ds))))\n",
    "        encoded_texts.append(x[0])\n",
    "        targets.append(x[1])\n",
    "\n",
    "    print(encoded_texts[1][0][:10])\n",
    "    print(targets[1][0][:10])\n",
    "\n",
    "    # make folder for prepared dataset for specific BERT model\n",
    "    os.makedirs(dataPath + modelName, exist_ok=True)\n",
    "\n",
    "    # store\n",
    "    for i, name in enumerate(('train', 'valid', 'test')):\n",
    "        with open(dataPath + f'{modelName}/{name}_data.pkl', 'wb') as f:\n",
    "            pickle.dump((encoded_texts[i], targets[i]), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the processed dataset from file\n",
    "# also create chunks of seqLen with padding\n",
    "def getPreparedDataset(prefix):\n",
    "    def pad_and_chunk(array, chunk_size: int):\n",
    "        padded_array = np.zeros(len(array) + (chunk_size - len(array) % chunk_size), dtype=\"int32\")\n",
    "        padded_array[: len(array)] = array\n",
    "        return np.array(np.split(padded_array, len(padded_array) / chunk_size))\n",
    "\n",
    "    with open(dataPath + modelName + \"/\" + prefix + \"_data.pkl\", 'rb') as f:\n",
    "        texts, targets = pickle.load(f)\n",
    "        X = [word for t in texts for word in t]\n",
    "        Y = [t for ts in targets for t in ts]\n",
    "    return pad_and_chunk(X, seqLen), pad_and_chunk(Y, seqLen)\n",
    "\n",
    "trainX, trainY = getPreparedDataset(\"train\")\n",
    "valX, valY = getPreparedDataset(\"valid\")\n",
    "testX, testY = getPreparedDataset(\"test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(trainX.shape)\n",
    "print(valX.shape)\n",
    "print(testX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom loss function which replaces the -1 labels with 0\n",
    "#  so it can be used to calculate the sparse categorical crossentropy\n",
    "def customSparseCategoricalCrossEntropy(y_actual,y_pred):\n",
    "    y_actual = tf.where(y_actual == -1, 0, y_actual)\n",
    "    loss=kb.sparse_categorical_crossentropy(y_actual, y_pred)\n",
    "    \n",
    "    # mean is not really needed\n",
    "    #loss=kb.mean(loss, axis=-1)\n",
    "    #loss=kb.mean(loss, axis=-1)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model construction\n",
    "\n",
    "# input layer (batchSize, seqLen)\n",
    "bert_input = tf.keras.Input(shape=(seqLen), dtype='int32', name='bert_input')\n",
    "\n",
    "# get embeddings for each token from BERT layer (batchSize, seqLen, 768)\n",
    "x = TFBertModel.from_pretrained(\"bert-base-uncased\")(bert_input)[0]\n",
    "\n",
    "# do dropout\n",
    "x = tf.keras.layers.Dropout(dropout, name=\"dropout1\")(x)\n",
    "\n",
    "# fully connected layer 768 -> 768x2\n",
    "x = tf.keras.layers.Dense(1536, name='linearExtra')(x)\n",
    "\n",
    "# do dropout\n",
    "x = tf.keras.layers.Dropout(dropout, name=\"dropoutExtra\")(x)\n",
    "\n",
    "# relu activation on fully connected layer output\n",
    "x = tf.keras.layers.Activation('relu', name='relu')(x)\n",
    "\n",
    "# fully connected output layer\n",
    "x = tf.keras.layers.Dense(numLabels, name='linear')(x)\n",
    "\n",
    "# sigmoid activation on output layer\n",
    "model_out = tf.keras.layers.Activation('sigmoid', dtype='float32', name='softmax')(x)\n",
    "\n",
    "model = tf.keras.Model(inputs=bert_input, outputs=model_out, name='model')\n",
    "model.compile(\n",
    "    tf.keras.optimizers.Adam(learning_rate=learningRate),\n",
    "    loss=customSparseCategoricalCrossEntropy,\n",
    "    metrics=tf.keras.metrics.SparseCategoricalAccuracy()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the model architecture\n",
    "model.summary()\n",
    "tf.keras.utils.plot_model(model, show_shapes=True, dpi=76)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training the data\n",
    "history = model.fit(\n",
    "    trainX, trainY,\n",
    "    #validation_split=0.2,\n",
    "    epochs=epochs,\n",
    "    batch_size=batchSize,\n",
    "    validation_data=(valX, valY)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the fitting history\n",
    "history.history\n",
    "\n",
    "# plot the loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluates the predicted results\n",
    "def evaluateResults(Ytrue, Ypred):\n",
    "    \n",
    "    # apply mask to ignore subwords\n",
    "    wordMask = Ytrue != -1\n",
    "    Ypred = Ypred[wordMask]\n",
    "    Ytrue = Ytrue[wordMask]\n",
    "            \n",
    "    print(classification_report(Ytrue, Ypred, target_names=labelNames))\n",
    "    \n",
    "    confusionMatrix = confusion_matrix(Ytrue, Ypred)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=confusionMatrix, display_labels=labelNames)\n",
    "    print(\"Confusion matrix:\")\n",
    "    disp.plot()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate output on validation data\n",
    "testPredict = model.predict(testX)\n",
    "\n",
    "# select best class for each token\n",
    "resulttestY = testPredict.argmax(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluateResults(testY.flatten(), resulttestY.flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the data for testing\n",
    "Xtest, Ytest, testTokens = loadDataFromTEDtalkDataset(\"IWSLT2012data/test2011\", tokenizer)\n",
    "Xtestasr, Ytestasr, testasrTokens = loadDataFromTEDtalkDataset(\"IWSLT2012data/test2011asr\", tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate output on test data\n",
    "testPredict = model.predict(Xtest)\n",
    "testPredictasr = model.predict(Xtestasr)\n",
    "\n",
    "# select best class for each token\n",
    "resultYtest = testPredict.argmax(axis=1)\n",
    "resultYtestasr = testPredictasr.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Test set:\")\n",
    "evaluateResults(Ytest, resultYtest)\n",
    "\n",
    "print(\"Test ASR set:\")\n",
    "evaluateResults(Ytestasr, resultYtestasr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate on test data\n",
    "#resultsTest = model.evaluate(Xtest, Ytest, batch_size=batchSize)\n",
    "#print(\"test loss, test acc:\", resultsTest)\n",
    "\n",
    "#resultsTestasr = model.evaluate(Xtest, Ytest, batch_size=batchSize)\n",
    "#print(\"test (asr) loss, test (asr) acc:\", resultsTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the restoration on the test sets\n",
    "# get results\n",
    "#restoreTest = model.predict(Xtest)\n",
    "#restoreTestasr = model.predict(Xtestasr)\n",
    "\n",
    "# select best class for each token\n",
    "#resultYtest = restoreTest.argmax(axis=1)\n",
    "#resultYtestasr = restoreTestasr.argmax(axis=1)\n",
    "\n",
    "# reconstruct text with predicted tokens\n",
    "#resTextTest = reconstructText(testTokens, resultYtest, tokenizer)\n",
    "#resTextTestasr = reconstructText(testasrTokens, resultYtestasr, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(resTextTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(resTextTestasr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example restoration on paragraph from a story\n",
    "#toPredict = \"Then he sat down and began to reflect. In the morning he must find seconds. Whom should he choose? He searched his mind for the most important and celebrated names of his acquaintance. At last he decided on the Marquis de la Tour-Noire and Colonel Bourdin, an aristocrat and a soldier; they would do excellently. Their names would look well in the papers. He realised that he was thirsty, and drank three glasses of water one after the other; then he began to walk up and down again. He felt full of energy. If he played the gallant, showed himself determined, insisted on the most strict and dangerous arrangements, demanded a serious duel, a thoroughly serious duel, a positively terrible duel, his adversary would probably retire an apologist.\"\n",
    "#predictText(toPredict, model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def insertTarget(x, segment_size):\n",
    "    # creates segments of surrounding words for each word in x.\n",
    "    # inserts a zero token ([PAD]) halfway the segment, right after the inserted token.\n",
    "    # for the first segmentSize/2 tokens, the end of the data is used, as if the text loops\n",
    "    X = []\n",
    "    x_pad = x[-((segment_size-1)//2-1):]+x+x[:segment_size//2]\n",
    "\n",
    "    for i in range(len(x_pad)-segment_size+2):\n",
    "        segment = x_pad[i:i+segment_size-1]\n",
    "        segment.insert((segment_size-1)//2, 0)\n",
    "        X.append(segment)\n",
    "\n",
    "    return np.array(X)\n",
    "\n",
    "def encodeRawText(text):\n",
    "    # splits the text on spaces and creates labels for each created token\n",
    "    # the resulting token list will have no punctuation anymore\n",
    "    splitOnSpace = text.split(' ')\n",
    "    X = []\n",
    "    Y = []\n",
    "    \n",
    "    for word in splitOnSpace:\n",
    "        if len(word) > 0: # skip empty tokens\n",
    "            # look for tokens at the end of the word\n",
    "            # remove if found when appending to X\n",
    "            # also make everything lowercase\n",
    "            if word[-1] == '.':\n",
    "                X.append(word.lower()[:-1])\n",
    "                Y.append(LABEL_PERIOD)\n",
    "            elif word[-1] == ',':\n",
    "                X.append(word.lower()[:-1])\n",
    "                Y.append(LABEL_COMMA)\n",
    "            elif word[-1] == '?':\n",
    "                X.append(word.lower()[:-1])\n",
    "                Y.append(LABEL_QUESTION)\n",
    "            else:\n",
    "                X.append(word.lower())\n",
    "                Y.append(LABEL_NOTHING)\n",
    "                \n",
    "    return X, Y\n",
    "\n",
    "def prepareDataForModel(words, labels, tokenizer):\n",
    "    # returns a list of segments of token IDs for X\n",
    "    #  and a list of label tokens for y, corresponding to the segment in X\n",
    "    #  and a list of the unsegmented token IDs, for easier reconstruction\n",
    "    X = []\n",
    "    Y = []\n",
    "    for word, label in zip(words, labels):\n",
    "        y = [label]\n",
    "        # retokenize x\n",
    "        x = tokenizer.wordpiece_tokenizer.tokenize(word)\n",
    "        # encode x\n",
    "        x = tokenizer.convert_tokens_to_ids(x)\n",
    "\n",
    "        # do not add if tokenize failed\n",
    "        if len(x) > 0:\n",
    "            # if multiple tokens, create multiple labels of 0\n",
    "            #  set the last one to the real label\n",
    "            if len(x) > 1:\n",
    "                y = (len(x)-1)*[0]+y\n",
    "            X += x\n",
    "            Y += y\n",
    "                \n",
    "    # create segments for X, and return together with Y, and the unsegmented tokens\n",
    "    # return as Numpy array\n",
    "    return np.array(insertTarget(X, segmentSize)), np.array(Y), X\n",
    "\n",
    "def getTokenFromSegment(segment):\n",
    "    # is always at the same place of the segment (assumes even numbers!)\n",
    "    return segment[segmentSize//2-2]\n",
    "\n",
    "def reconstructText(tokenList, labels, tokenizer):\n",
    "    # reconstructs text by detokenizing and applying the given labels\n",
    "    tokens = tokenizer.convert_ids_to_tokens(tokenList)\n",
    "    \n",
    "    reconstructedText = \"\"\n",
    "    for tok, label in zip(tokens, labels):\n",
    "        # no space in between if second token starts with '##'\n",
    "        if tok.startswith(\"##\"):\n",
    "            reconstructedText += tok[2:]\n",
    "        else:\n",
    "            reconstructedText += \" \" + tok\n",
    "        \n",
    "        # add the punctuation from the label\n",
    "        if label == LABEL_COMMA:\n",
    "            reconstructedText += \",\"\n",
    "        elif label == LABEL_PERIOD:\n",
    "            reconstructedText += \".\"\n",
    "        elif label == LABEL_QUESTION:\n",
    "            reconstructedText += \"?\"\n",
    "                \n",
    "    reconstructedText = reconstructedText[1:] # skip the first space\n",
    "    \n",
    "    return reconstructedText\n",
    "    \n",
    "def loadDataFromFile(path):\n",
    "    # creates a dataset from a text file, which can be used on the model\n",
    "    # returns a list of segments of token IDs for X\n",
    "    #  and a list of label tokens for y, corresponding to the segment in X\n",
    "    #  and a list of the unsegmented token IDs, for easier reconstruction\n",
    "    text = \"\"\n",
    "    with open(path, 'r') as file:\n",
    "        text = file.read()\n",
    "        \n",
    "    X, Y = encodeRawText(text)\n",
    "    dataX, dataY, dataTokens = prepareDataForModel(X, Y, tokenizer)\n",
    "    \n",
    "    return dataX, dataY, dataTokens\n",
    "\n",
    "def loadDataFromTEDtalkDataset(path, tokenizer):\n",
    "    # loads the data from the TED talk dataset, which is already pre-processed\n",
    "    # returns a list of segments of token IDs for X\n",
    "    #  and a list of label tokens for y, corresponding to the segment in X\n",
    "    #  and a list of the unsegmented token IDs, for easier reconstruction\n",
    "    X = []\n",
    "    Y = []\n",
    "    with open(path, \"rb\") as file:\n",
    "        for line in file:\n",
    "            # dataset uses \\r\\n for newlines\n",
    "            word, punc = line.decode('utf-8', errors='ignore').replace('\\r\\n', '').split('\\t')\n",
    "            # encode y\n",
    "            y = [punctEncode[punc]]\n",
    "            # retokenize x\n",
    "            x = tokenizer.wordpiece_tokenizer.tokenize(word)\n",
    "            # encode x\n",
    "            x = tokenizer.convert_tokens_to_ids(x)\n",
    "            \n",
    "            # do not add if tokenize failed\n",
    "            if len(x) > 0:\n",
    "                # if multiple tokens, create multiple labels of 0\n",
    "                #  set the last one to the real label\n",
    "                if len(x) > 1:\n",
    "                    y = (len(x)-1)*[0]+y\n",
    "                X += x\n",
    "                Y += y\n",
    "                \n",
    "    # create segments for X, and return together with Y, and the unsegmented tokens\n",
    "    # return as Numpy array\n",
    "    return np.array(insertTarget(X, segmentSize)), np.array(Y), X\n",
    "\n",
    "def predictText(text, model, tokenizer):\n",
    "    # predicts the punctuation for a given text and print the resulting text\n",
    "    \n",
    "    # pre-process\n",
    "    X, Y = encodeRawText(text)\n",
    "    dataX, dataY, dataTokens = prepareDataForModel(X, Y, tokenizer)\n",
    "    \n",
    "    # get results\n",
    "    results = model.predict(dataX)\n",
    "    \n",
    "    # select best class for each token\n",
    "    resultY = results.argmax(axis=1)\n",
    "    \n",
    "    # reconstruct text with predicted tokens\n",
    "    resText = reconstructText(dataTokens, resultY, tokenizer)\n",
    "    \n",
    "    print(resText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "class Generator(Sequence):\n",
    "    \n",
    "    def __init__(self, X, Y, isTrain=False):\n",
    "        self.encoded_texts = np.array(X)\n",
    "        self.targets = np.array(Y)\n",
    "        self.isTrain = isTrain\n",
    "        self.indexs = np.arange(len(X))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.encoded_texts)//seqLen - 1\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        #shift = np.random.randint(seqShift) - seqShift // 2\\\n",
    "        #    if self.isTrain else 0\n",
    "\n",
    "        start_idx = index * seqLen# + shift\n",
    "        start_idx = max(0, start_idx)\n",
    "        end_idx = start_idx + seqLen\n",
    "        return np.array([self.encoded_texts[start_idx: end_idx]]), np.array([self.targets[start_idx: end_idx]])\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "class DataGen(Sequence):\n",
    "    def __init__(self, batch_size, isTrain=False, X, Y):\n",
    "        self.encoded_texts = np.array(X)\n",
    "        self.targets = np.array(Y)\n",
    "        self.isTrain = isTrain\n",
    "        self.indexs = np.arange(len(X))\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encoded_texts)//seqLen - 1\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        start_idx = i * seqLen# + shift\n",
    "        start_idx = max(0, start_idx)\n",
    "        end_idx = start_idx + seqLen\n",
    "        return self.encoded_texts[start_idx: end_idx], self.targets[start_idx: end_idx]\n",
    "        \n",
    "        batch = self.samples[i * self.batch_size:(i + 1) * self.batch_size]\n",
    "        return self.preproc.process(*zip(batch))\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        shuffle(self.samples)\n",
    "\"\"\"\n",
    "\n",
    "#train_gen = Generator(valX, valY, isTrain=True)\n",
    "#data_gen = DataGen(batchSize, True, valX, valY)\n",
    "#myit = iter(train_gen)\n",
    "#print(next(myit))\n",
    "\n",
    "#X1 = []\n",
    "#X2 = []\n",
    "#X3 = []\n",
    "#Y1 = []\n",
    "\n",
    "#for i in range(64):\n",
    "#    a, b = next(myit)\n",
    "#    X1.append(a)\n",
    "#    X2.append([1] * seqLen)\n",
    "#    X3.append([0] * seqLen)\n",
    "#    Y1.append(b)\n",
    "\n",
    "#train_data = train_data.shuffle(shuffle_buffer_size)\n",
    "#train_data = train_data.batch(batch_size)\n",
    "\n",
    "# print(train_gen)\n",
    "\n",
    "#print(Y1)\n",
    "\n",
    "# test\n",
    "#X, y = next(iter(train_data))\n",
    "#print(X[0].numpy())\n",
    "\n",
    "#print(tokenizer.decode(X[0].numpy()))\n",
    "#print()\n",
    "#print(tokenizer.decode(X[1].numpy()))\n",
    "#print()\n",
    "#print(tokenizer.decode(X[2].numpy()))\n",
    "#print()\n",
    "#print(tokenizer.decode(X[3].numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testX, testY = getPreparedDataset(\"test\")\n",
    "\n",
    "#print(testX.shape)\n",
    "\n",
    "#results = model.predict(testX)\n",
    "\n",
    "# select best class for each token\n",
    "#resultY = results.argmax(axis=-1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text preprocessing demo\n",
    "if False:\n",
    "    s = \"Tyranosaurus: kill me? Not enough, -- said the co-pilot -- ...\"\n",
    "    s = clean_text(s)\n",
    "    X, Y = encodeRawText(s)\n",
    "    data, targets = create_target(s)\n",
    "    print(X)\n",
    "    print(Y)\n",
    "    print(data)\n",
    "    print(targets)\n",
    "    print([tokenizer._convert_id_to_token(d) for d in data[1:-1]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
